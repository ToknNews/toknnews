ToknNews System Architectural Breakdown
Overview of System Architecture
ToknNews is an automated news broadcasting system with multiple components working in a pipeline. At a high level, it ingests real-time news data, compiles that into a structured “scene,” decides what type of segment to run (e.g. news update, intro reset, ad break, etc.), and uses an AI-driven Script Engine to generate a broadcast script with dialog for a cast of persona-based anchors. A continuous Broadcast Loop orchestrates these steps in cycles, producing serialized “broadcast blocks” (with text and audio) that make up the live show[1]. The architecture balances scheduled timing rules (when to introduce segments or breaks) with content-driven logic (e.g. detecting breaking news or high-impact stories) to mimic a live newsroom broadcast.
Repository and File Structure
The codebase is organized into clear sections reflecting the pipeline stages:
backend/live/ – Real-time ingestion and scene compilation. This includes the ingestors for various sources and the scene compiler that aggregates news into a structured format. For example, run_cycle.py in this directory repeatedly triggers the unified ingestor and scene compiler under a process manager (PM2)[2].
backend/script_engine/ – The Script Engine and related logic for content generation. This contains submodules for personas, dialogue building, and the Programming Director (in director/). Key components here are the timeline builder, OpenAI integration (openai_writer.py), and persona definitions. (A public version of the script engine logic is available in the toknnews_engine_public repository[3].)
backend/script_engine/persona/ – Persona-specific logic, such as timeline_builder.py for assembling the sequence of lines in a segment, and line_builder.py for constructing individual lines.
backend/script_engine/director/ – The Programming Director (PD) logic, handling segment selection, pacing, ads, and special interrupts. This includes files like director_brain.py (main PD logic), pd_controller.py (simplified PD API for the engine), and rules modules (e.g. breaking_logic.py, ad_logic.py, hijinx_rules.py).
backend/broadcast/ – The Broadcast Loop orchestrator. Notably, broadcast_loop.py runs as the live loop tying everything together: it triggers ingestion/compilation, calls the PD to choose the next segment, invokes the Script Engine to generate that segment’s script, and saves the output to the broadcast data store[1][4].
backend/rest/ – A lightweight REST API (FastAPI) exposing endpoints to trigger ingestion or compilation externally (for instance, an /ingest/run endpoint that kicks off the hybrid_ingestor.py process[5]). This can be used for manual or external control of the pipeline.
Within the data directory (e.g. /var/www/toknnews-live/data/), the system stores intermediate and output files, such as raw source data dumps, compiled scenes (e.g. latest_scene.json), and finalized broadcast blocks (JSON files containing the script and links to rendered audio).
Data Ingestion and Scene Compilation
Ingestion Pipeline: ToknNews gathers news from multiple sources in each cycle. The hybrid ingestor (run via hybrid_ingestor.py) fetches data from RSS feeds, APIs (e.g. CoinDesk, Dexscreener), social platforms (Reddit), etc. The design merges these into a unified set of “story inputs.” The process is automated by backend/live/run_cycle.py, which on every cycle (e.g. every 60 seconds) executes the ingestor and then the compiler, logging a heartbeat for monitoring[6][7]. This modular approach allows adding or removing sources easily; for instance, the system currently lists sources like rss, reddit, pumpfun, moralis and others in its API[8], indicating an extensible ingestion framework.
Scene Compilation: After raw data is fetched, the Scene Compiler (scene_compiler_live.py) processes those inputs into a structured scene that the Script Engine can use. A “scene” generally represents a single news story or update, enriched with metadata. For example, the compiler will load recent RSS headlines (and similarly from other sources) and create a list of scene entries. Each scene entry might include: timestamp, a primary character (anchor) assigned, the news topic/headline, a short summary or extracted key points, source info, and an initial status[9][10]. The compiler deduplicates stories (to avoid repeating news within a recent window) by hashing titles and skipping those already seen in the last 24 hours[11][12].
For each new headline, the compiler invokes the Script Engine to generate a script block (dialogue) for that story. This is done via a call to generate_anchor_script, which wraps the Script Engine’s generate_script function[13][14]. The result is a structured script for that news item (we’ll detail this generation below). The compiled scene is then saved: it gets appended to a master list (scenes.json as an archive of recent scenes) and also written out as the current latest_scene.json[15]. The latest scene file represents the most recent news story ready to be broadcast.
The compilation step also updates some analytics for internal use or dashboard: e.g. it updates a daily counts file (how many stories compiled per day) and maintains a snapshot of recent scenes for quick reference[16][17]. Additionally, each scene is stored in a SQLite DB for persistence[18]. These are part of the “brain ingestion stack” – essentially feeding the system’s memory with fresh content.
At the end of ingestion+compilation, we have an up-to-date latest_scene.json containing the newest story and an AI-generated script for it. This is the input to the Programming Director in the broadcast loop.
Programming Director Logic and Timing Rules
The Programming Director (PD) acts as the decision-maker for segment sequencing and pacing of the broadcast. It looks at the current content (and the show’s state) to decide what type of segment should happen next. Segments could be things like a standard news report, a special intro or reset, an ad break, or a fun “hijinx” interruption. The PD encapsulates the business logic and timing rules that give the broadcast a realistic flow.
In the continuous broadcast scenario, the PD’s logic (implemented in director_brain.py) runs every cycle to evaluate what to do next[19][20]. Key aspects of PD logic include:
Intro Segments: Ensures the show opens properly. At the very start of a broadcast (or after a major reset), PD will schedule a “show_intro” segment. For example, if no intro has played yet, the router will return show_intro which triggers a proper opening sequence (welcome by the hosts)[21]. PD uses a flag intro_played in its state to avoid repeating the intro too often[22].
Breaking News Overrides: PD checks if any incoming story is marked as breaking. If so, it can override normal scheduling and immediately force a breaking segment type[23]. (This would prompt the script engine to generate a different tone of delivery, presumably). Breaking news is detected via rules in breaking_logic (e.g. based on story importance or keywords) and will take precedence over other segments.
Chip Reset (Show Reset): This is a mechanism to periodically “reset” the broadcast and bring the lead anchor (Chip) back to re-center the narrative. The PD determines if conditions warrant a reset – for example, after a certain amount of time or escalation of intensity. If should_reset_show(...) returns true (and it’s allowed based on cooldown), PD returns a special segment payload of type "chip_reset"[24][25]. A Chip Reset causes the system to pause the normal flow and run a dedicated reset script (Chip providing a recap or fresh intro), which helps the broadcast feel structured (like top-of-hour resets). The code also sets a cooldown by using reset_suppression_cycles so that after a reset, no new reset will trigger for a few cycles[26][27]. This prevents back-to-back resets. In the broadcast loop, if a chip_reset is triggered, it’s handled immediately and the cycle returns early (after writing out that reset block)[28].
Hijinx & Fun Interruptions: To keep the show lively, the PD can occasionally inject a humorous or unexpected segment. Two special personas handle this: Vega (code-name for chaotic “energy shift” interludes) and Bitsy (a meta, glitchy commentator). The PD computes a hijinx probability each cycle based on factors like time of day or sentiment[29]. If a random draw falls under this probability, it will trigger a hijinx action via the Hijinx Engine. For example, PD picks a hijinx_line (some humorous one-liner) and decides if it’s a Bitsy meta interruption or a Vega pad. Bitsy’s lines are identified by starting with "Bitsy:", in which case PD sets up a bitsy_meta segment[30]. Otherwise, any other hijinx line becomes a vega_pad segment[30]. In both cases the chosen line is stored in PD state and will be handed to the Script Engine to produce the interruption skit. The PD ensures these don’t happen during certain segments (e.g. not during a breaking news or intro). This randomness and meta-commentary (like a character “breaking the fourth wall”) add a dynamic, unpredictable flavor to the broadcast.
Ad Breaks: The PD also handles when to take promotional breaks. The logic (in ad_logic.should_run_ad) likely checks if enough time has passed since the last ad and if the current moment is suitable (e.g. not immediately after a reset or breaking news). If conditions fit, PD will return an ad_break segment[31]. This signals the script engine to generate an advertisement or promotional segment. The PD updates last_promo_time and such to track when ads ran[31]. (At the moment, the ad content itself may be a placeholder or needs integration, but the hook for ads is in place.)
Regular News Segments: If none of the special cases (breaking, reset, hijinx, ad) override, the PD defaults to selecting the next content segment via a router. The Segment Router examines the current story queue and state to pick what’s next. In the simplified PD controller (used for one-off script generation), this might simply return "headline" for a normal news story[32]. In the continuous loop PD, a more advanced router could pick among options like “news”, “panel discussion”, etc., but currently it often returns "news" by default or based on rotation. In director_brain.py, after all other checks, PD calls route_next_segment to obtain a next_segment type and logs it[33][34]. That result becomes PD’s decision – e.g., "news" meaning proceed with a standard news story script. PD records this in its state (last_segment) and the segment history.
Timing Rules & State: The PD maintains a state (DirectorState) that tracks timing of last occurrences: when the last intro ran, last reset, last banter, last ad, etc. This is used to enforce spacing rules. For instance, an intro might only be allowed if a certain amount of time has passed since the last intro. In the broadcast loop initialization, they explicitly backdate some of these times to simulate that a show has already been running (so that on startup it doesn’t immediately trigger intros back-to-back)[35][36]. The PD also tracks a segment_history list (recent segments) to avoid repetitive patterns and aid decisions[37]. There’s also an escalation_level representing the intensity/importance of recent news, which the PD computes each cycle (via compute_escalation_level) to decide on resets or tone adjustments[38]. If a reset occurs, PD temporarily forces escalation_level = 0 for a couple cycles to let things calm down[26].
Cycle Loop: The broadcast loop (broadcast_loop.py) ties timing together by running continuously and sleeping a short interval between cycles. Currently it’s set to iterate every 10 seconds[39], meaning the system evaluates new content and potentially produces a new broadcast segment every 10 seconds (this can be tuned). The ingestion cycle (which runs separately via run_cycle.py) is slower (e.g. 60s) because news updates don’t happen every second. In a deployed scenario, one might decouple these or synchronize them based on need – e.g. ingest continuously, but broadcast can iterate faster if there’s a backlog of stories.
In summary, the PD ensures that the broadcast isn’t just a flat loop of news stories. It injects structure: an intro at the start, periodic resets for pacing, ad breaks for monetization, and occasional fun interruptions – all based on timing rules and content cues. This creates a realistic flow akin to a live news channel.
Script Generation and Persona Engine
Once the PD selects a segment and the relevant story (or directive) to cover, the Script Engine takes over to actually generate the content of that segment. ToknNews uses a cast of AI-powered anchor personas to present the news, each with distinct voices and perspectives. The script generation stage produces a timed sequence of dialog lines for those personas, possibly along with metadata to render audio/visual output.
Persona Definitions: Each anchor persona is defined with a unique profile (sometimes called the “character bible”). This includes their name, role, specialty domain, voice style, and mannerisms. For example, the lead anchor Chip (Chip Blue) is defined as a lead_anchor who covers all domains with a balanced tone and professional, explanatory style[40]. Others like Reef or Lawson are regular anchors or analysts with specific domain focuses (DeFi, regulation, etc.) and distinct tones (e.g. Reef is upbeat and momentum-driven, Lawson is dry and skeptical)[41][42]. There are also “specialist” roles like Ledger (on-chain analytics), Cap (DeFi tech), Neura (AI/tech), etc., each mapped to certain topic domains[43][44]. Notably, the system has two “wildcard” characters: Vega Watt (role: chaos) who provides energetic, unpredictable commentary, and Bitsy Gold (role: meta) who makes cutesy, meta-aware interruptions[45][46]. These profiles inform both which anchor is chosen for a story and how they speak.
Anchor Selection for a Story: Given a news headline (and any PD directives like primary domain or tone shift), the system selects one or two anchors to cover the segment. Typically, Chip as host is involved in most segments, often accompanied by another anchor for duo commentary. The PD (or pd_controller.select_anchors) uses the character bible info to score which persona best fits the story’s domain and context[47][48]. For example, if the story’s primary domain is determined to be “macro” (macroeconomics), an anchor like Bond (macro strategist) would score high and likely be selected as a primary or secondary voice. Roles also influence this: a primary_anchor role (like Chip) might get a baseline priority boost[48]. The selection logic can also enforce duos: for certain domains, a predefined pairing is recommended (e.g. if domain is “volatility”, the duo might be Rex and Reef, whereas “regulation” might pair Lawson and Bond)[49][50]. The selection function returns a list of one or two anchors for the segment[51][52]. In a continuous show mode, Chip might always be present and the second anchor rotates based on story domain, whereas in single-story generation mode, Chip might not be explicitly added unless needed (depending on show_intro or not).
Timeline Building: The Timeline Builder (timeline_builder.py) is responsible for constructing the scripted dialogue flow once anchors are chosen. It creates a sequence of blocks, each block representing a line spoken by a character (with associated metadata like which voice to use, timestamp, etc.). A typical news segment timeline might include: an intro line (Chip or Vega opening), a primary analysis by the main anchor, a secondary reaction by the second anchor, possibly a back-and-forth exchange (a few turns of crosstalk), a follow-up question from Chip, and a closing or toss to the next segment. The timeline builder uses a combination of templated logic and GPT-powered generation to fill in these parts.
Some elements are straightforward: for instance, if it’s the very first segment of the show (show_intro segment), the timeline builder will include a Vega ident line and a Chip greeting. The Vega ident is basically a static welcome phrase (“Good morning, and welcome to TOKN News.”)[53], and Chip’s opening adjusts to the time of day (“Good morning/afternoon/evening, I’m Chip. Let’s get straight into today’s top story.”)[54]. These give a polished intro feel.
For the content of the news discussion, the Script Engine relies heavily on OpenAI GPT models to generate natural-sounding lines in each anchor’s voice. The openai_writer.py module defines several prompt-generation functions for different types of lines:
Analysis Lines: A primary anchor’s summary or analysis of the news. The gpt_analysis() function crafts a prompt instructing the model to write a few sentences analyzing the given headline and its summary[55]. It injects the persona profile (voice, style guidelines) via a persona_prompt and then asks for up to 3 sentences of analysis on the headline and context. This yields the main explanatory content from that anchor.
Transition Lines: If needed, gpt_transition() can generate a one-sentence transitional line to segue into the next topic[56]. This might be used for intros or outros.
Anchor Reaction Lines: gpt_anchor_react() produces a brief follow-up sentence for an anchor to react to the headline’s “tension”[57]. This could be used if, say, the co-anchor wants to chime in briefly after the main analysis.
Duo Crosstalk: When two anchors are both engaged (a primary and a secondary), the system uses gpt_duo_line() to generate responsive dialogue lines for each, creating a conversational back-and-forth. The duo generation is more complex – it provides the model with context including who the speaker is and who they are responding to, the domain of the story, what the last line said, and even a snippet of “memory” context relevant to the speaker[58][59]. It sets rules such as: if multi-sentence is allowed or not, that the response should directly address the last line, maintain some tension or alignment appropriately, avoid repeating specific jargon or nouns the other person just used, and stay in character[59][60]. This helps ensure the conversation sounds natural and the two anchors aren’t just echoing each other. The timeline builder calls gpt_duo_line in alternation – e.g. first for the primary anchor responding to the secondary[61], then for the secondary responding to the primary[62], and possibly additional rounds (analysis round, reaction round, etc. as seen in the code sequence)[63][64].
Chip Follow-up: After a duo exchange, the host Chip often steps back in with a follow-up question or clarifying prompt. This is generated by gpt_chip_followup(). This prompt is carefully crafted to consider the context of the discussion and any PD flags (indicators like if the topic is tragic, or highly volatile, or socially trending)[65][66]. The rules for Chip’s follow-up ensure that in news mode he is factual and direct, and in latenight mode (if the show had a lighter mode) he could be witty but never inappropriate[67]. For example, if tragedy_block is true (meaning the story is a tragedy), Chip’s follow-up will avoid any humor and remain empathetic[67]. If volatility_risk is high, his question will be about market risk or caution[68]. The prompt explicitly says “DO NOT summarize the headline; add something NEW.” – forcing Chip’s follow-up to push the discussion forward, not just rehash. It also forbids filler agreements like “absolutely” to keep Chip’s authority. The output is a single sentence question or statement from Chip to cap that segment of discussion.
Toss to Next Anchor/Segment: At segment end, Chip might “toss” the coverage to another anchor or segment. gpt_chip_toss() generates this hand-off line[69]. The rules for the toss are straightforward: in news mode it should be crisp and professional, and in a hypothetical late-night mode, it can be more playful[70]. It must be only one sentence, use the next anchor’s name, and contain no fluff like “Alright, moving on”[70] – the line should cleanly pass the baton, e.g., “Rex, take us deeper into the volatility drivers here.” or “Ivy, what’s the regulatory angle on this?”. This ensures continuity into the next segment.
All these pieces are assembled by the timeline builder into an ordered list (timeline) of script blocks. Each block is a dictionary containing the line text, the character speaking, and a voice_id (which maps to a specific TTS voice for that character)[71][72]. Timestamps or sequence ordering info may also be attached. The timeline often distinguishes block types (intro, analysis, reaction, etc.) for any post-processing.
Audio Rendering: Once the timeline (text for each line) is ready, the Script Engine calls the audio renderer to produce audio files for each block or the whole segment. In script_engine_v3.generate_script, after building the timeline, they call render_audio_blocks(scene_id, package["audio_blocks"]) which likely communicates with a TTS service to get audio clips[73]. The result is an audio_file (maybe a combined audio for the segment, or reference to where audio chunks are stored) that is included in the returned script package[74]. Each character’s voice_id ensures the correct synthetic voice is used (e.g., Chip’s lines get Chip’s voice, Reef’s lines get Reef’s voice, etc.). The final output of generate_script (for a given headline/segment) is a JSON-friendly structure containing the original headline, a synthesis/summary, the timeline of lines, the list of audio_blocks or audio segments, and an audio_file path for the composite audio[74].
To summarize this stage: the Persona Engine leverages structured prompts and the predefined speaking styles of each character to produce a multi-voice narrative for each news story. It ensures each anchor stays in character (using their typical phrases and tone) and that interactions feel coherent. The combination of templated rules (e.g. avoiding repeated jargon) with AI-generated text yields a balance of consistency and creativity in the broadcast. The end result is a script sequence ready to be delivered on air, with accompanying audio, which the broadcast loop then packages as a broadcast block (e.g., writing out block_{timestamp}.json and the audio file to the data/broadcast folder[75]).
Rolling Brain and Memory Ingestion Stack
ToknNews includes a concept of a “Brain” – a central knowledge/state store that can be used by the system to maintain context over time. Currently this is in an early stage (v0.1) and serves primarily as a static configuration for anchors, but it’s designed to evolve into a dynamic, learning memory fed by the continuous ingestion of news (“brain ingestion stack”).
Rolling Brain: The rolling_brain.py defines a structure called the “RollingBrain” which holds the anchor metadata and trending context. In version 0.1, it returns a dictionary with an anchors section (each anchor’s key traits like domain specialties and a base weight) and placeholders for trending_topics, recent_events, and sentiment[76][77]. For example, anchors have weights (Chip has weight 8, Reef 7, etc.) which might represent their general importance or frequency of use[76]. The trending topics list and recent events are empty lists in this minimal version, and sentiment is an empty dict – these are meant to be populated by the live ingestion in the future[77]. The RollingBrain’s purpose is to provide a snapshot of the world’s state relevant to the news broadcast, which the system can use for smarter decisions (like which topics are hot, or how sentiment is shifting). The documentation notes for RollingBrain explicitly say it will later be replaced with an ingestion-fed rolling memory[78], implying that as news comes in, this brain object will update with counts of topic frequencies, recent big events, audience sentiment analysis, etc.
In practice, parts of the Script Engine already use the brain snapshot. For instance, the timeline builder calls get_brain_snapshot() and passes the resulting brain into functions that choose the primary anchor for a headline[79][80]. Currently, that selection logic uses simple keyword matching to assign a domain and then uses the anchor weights from the brain to pick who should lead[81][82]. Because the trending topics are not yet integrated, this is fairly static – but as the RollingBrain starts tracking which domains are trending (e.g., if a lot of DeFi stories are coming in, perhaps Reef’s weight or priority should increase dynamically), the anchor selection will become more adaptive to the news cycle.
Character Memory: Alongside the RollingBrain (which is more about global news context), the system is preparing to handle character-specific memory. In character_brain/brain_engine.py, there are functions to load and save a character_memory.json[83][84]. The intent is to allow each persona to have a memory of past interactions or facts they’ve stated. This could enable, for example, an anchor recalling a detail mentioned earlier in the broadcast, or maintaining a consistent viewpoint across segments. As of now, this memory system is just a scaffold – load_memory() returns an empty dict if no file exists[83], and generate_persona_line() mostly uses the character’s predefined style template to produce a line[85][86]. The code hints at future expansion: comments mention “future adaptive behavior” for memory and tone hooks[87]. The persona profile (potentially loaded from character_brain.json) includes things like emotional tone maps and style templates which could be combined with memory to influence responses[88][89]. For now, a simple example is included: if the character is Vega (the chaotic persona), and the story domain is technical (e.g. DeFi, macro, etc.), the code replaces Vega’s analysis line with a generic “whoa above my pay grade” vibe line[90][91] – in other words, Vega never gives technical analysis, staying in the lane of providing only atmosphere.
Brain Ingestion Stack (Future): Going forward, the plan is to have the ingestion pipeline feed into the “brain.” This likely means: as headlines are ingested and parsed, the system will update the RollingBrain’s trending_topics with the most common subjects or keywords in recent news, update recent_events with any major events (perhaps by importance or breaking status), and possibly gauge sentiment (maybe via sentiment analysis of headlines or social feeds). This information can then be used by the PD or script generation. For example, PD’s compute_escalation_level could use sentiment to gauge if the news overall is “tense” (high negativity or volatility), which might trigger a reset or a different tone. Or the script engine might use trending topics in the persona prompts to let anchors mention those contexts. The groundwork for this is laid out, as seen by those fields in RollingBrain[77] and references to “brain snapshot (summaries only)” in the Chip follow-up and toss prompts[92][93] (they pass a summary of the brain, likely not fully implemented yet).
In summary, the “brain” is the system’s memory and context center: currently it’s static config for anchors, but it is meant to evolve into a dynamic knowledge base updated by live data. This will make ToknNews smarter and more context-aware over time, ensuring the AI anchors have some persistence and the show responds to trending information holistically, not just one story at a time.
Outstanding To-Dos and Upcoming Patches
The ToknNews project is actively evolving, and several features are identified as in-progress or next steps in the codebase:
Dynamic Brain Integration: As noted, the RollingBrain is still static. A top priority is to fully integrate the ingestion pipeline with the brain. This means implementing the logic to update trending_topics, recent_events, and sentiment in real-time and adjusting anchor weights or PD behavior accordingly[77]. This will likely involve analyzing the compiled scenes or raw inputs to extract frequent topics and any sentiment signals. Once in place, anchor selection and tone could become much more adaptive (e.g. if “SEC lawsuit” is trending, the PD might favor Lawson or Ivy more often, etc.).
Enhanced Programming Director (PD) Logic: The current PD is labeled “minimal version” and the comments explicitly list planned enhancements: “segment routing, breaking logic, ad engine, hijinx rules, memory engine, etc.” to be added in later patches[94]. Some of these are partially implemented (breaking logic, ads, hijinx as described), but we can break down the to-dos:
Segment Routing & Variety: Introduce more segment types and smarter rotation. For example, panel discussions with 3+ anchors, deeper analysis segments, or user Q&A segments could be added. The routing logic can grow beyond the simple headline vs show_intro vs breaking options to incorporate these.
Refined Breaking News Detection: The logic could be improved to scan incoming stories for certain keywords or sources (e.g. an on-chain exploit alert) and categorize them as breaking with different urgency levels. Also, when breaking news happens, the script generation might switch to a different format (e.g. Chip delivering a short bulletin).
Ad Engine Content: While PD can trigger an ad_break, currently there isn’t a generator for actual ad content. A to-do is to create an Ad Content module or integrate with an external source so that when an ad break is triggered, the script engine produces a realistic ad or sponsor message. Ensuring the timing (perhaps every N minutes or after X stories) is also part of this.
Memory Engine & Continuity: Integrating the character memory system so that anchors remember earlier details or maintain conversational threads. This might involve storing key points from each scene in the brain memory and having the GPT prompts reference them for later segments (e.g., “as we discussed earlier, ...”). The groundwork is there (character_memory.json usage, etc.), but implementing it is an upcoming task.
Audience Interaction & Feedback Loops: Not explicitly in code, but a logical extension is using feedback (like if an anchor’s line falls flat or is too repetitive) to adjust styles on the fly. This could tie into sentiment analysis of social media (if sentiment in brain is negative, maybe tone down overly cheerful anchors on serious news).
Persona and Dialogue Refinements: Recent patches have introduced advanced capabilities like Chip’s follow-up questions and toss lines (referred to as “Patch 8” in comments[95]). Upcoming work will refine these:
Toss Engine Complete Integration: Ensure that at the end of every segment, if appropriate, Chip (or another host) delivers a toss to the next segment/anchor smoothly. This might involve coordinating with PD to know who the next anchor or topic is, or queuing up multiple stories.
Multi-anchor Crosstalk (Panel Discussions): Currently duo conversations (two anchors) are supported. A future improvement is to handle a “panel” of three or more anchors discussing a topic. This requires more complex turn-taking logic and perhaps a different prompt strategy to avoid chaos. There are signs of this being considered (e.g. code mentions follow_anchor in timeline builder, possibly for a third anchor’s follow-up line[96]), but it’s not fully implemented yet.
OpenAI Model Tuning: The prompts are designed for a particular model (there’s a reference to gpt-4o-mini as the model for Chip’s follow-up[97]). Future patches may experiment with different models or adjust temperature and style to get the best results. Also, prompt templates might be tweaked based on testing (to ensure anchors don’t produce unwanted phrases or stay within time limits).
User Interface and Public Integration: While not in the code we reviewed, an upcoming consideration is how these broadcast segments are delivered to end-users. The system currently produces JSON and audio files; building a front-end that assembles these into a live stream or publishes them (e.g., on a website or as a live audio feed) would be a next step outside the core engine. The repository structure hints at a live deployment in /var/www/toknnews-live, so ensuring the pipeline runs reliably in production (using the heartbeat mechanism and PM2) is an ongoing operational task.
Codebase Cleanup and Documentation: The existence of backup files and old versions in the repo suggests there’s ongoing refactoring. Completing the migration to the new Script Engine v3 (the “Step 3-G Integration” mentioned in code[98]), and removing deprecated code (like anything in director_OLD_BACKUP, or the older scene compiler versions in sandbox/) will be part of cleanup. Writing comprehensive documentation (the README is currently a placeholder) is also likely on the to-do list, so that new contributors or users can understand how to use the system.
In summary, many of the core pieces are in place, but future patches will focus on making the system more adaptive (via memory and brain ingestion), more versatile in content (ads, multi-anchor segments), and robust in production. The comments in code explicitly note these upcoming improvements, indicating a clear roadmap for development.
Next Steps – Continuing the Work
With the architectural context established, the next step is to continue from the previous working session’s progress. A logical continuation would be to start implementing the dynamic RollingBrain ingestion and memory integration, as this will unlock more intelligent behavior in the system.
For example, we might proceed by writing a module or function to aggregate topics from recent scenes and update rolling_brain.py accordingly. This could involve scanning the scenes.json for the most frequent domains or keywords in the last N stories and populating brain["trending_topics"], as well as capturing any notable recent event (perhaps the latest breaking story) for brain["recent_events"]. We should also consider updating the compute_escalation_level logic to factor in sentiment or repetition of negative news.
Next Session Prompt: Let’s now continue the development by focusing on the “brain ingestion” feature. We will design how the system can update its internal brain state with trending topics and sentiments from the incoming news. This will involve: identifying the data structures to capture topic frequency, deciding when/how to update the brain (possibly each cycle or on each scene compile), and then utilizing that updated brain in the PD or script generation.
For instance, a development task could be: “Implement a function to analyze the latest 50 scenes and update the RollingBrain’s trending_topics list (and possibly sentiment), then modify the anchor selection to use this info.”
Let’s proceed with that implementation, ensuring we integrate it smoothly with the existing pipeline and test that our anchors’ selection or tone adjusts based on the dynamic brain data.

[1] [4] [19] [20] [28] [35] [36] [37] [39] [75] broadcast_loop.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/broadcast/broadcast_loop.py
[2] [6] [7] run_cycle.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/live/run_cycle.py
[3] [73] [74] script_engine_v3.py
https://github.com/ToknNews/toknnews_engine_public/blob/7ee3859b19c34c26b3c6412f2a83b627161a5b94/script_engine/script_engine_v3.py
[5] [8] ingest.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/rest/routes/ingest.py
[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] scene_compiler_live.py~
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/live/scene_compiler_live.py~
[21] [22] [32] segment_router.py
https://github.com/ToknNews/toknnews_engine_public/blob/7ee3859b19c34c26b3c6412f2a83b627161a5b94/script_engine/director/segment_router.py
[23] [24] [25] [26] [27] [29] [30] [31] [33] [34] [38] [94] director_brain.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/script_engine/director/director_brain.py
[40] [41] [42] [43] [44] [45] [46] character_bible.json
https://github.com/ToknNews/toknnews_engine_public/blob/7ee3859b19c34c26b3c6412f2a83b627161a5b94/script_engine/character_brain/character_bible.json
[47] [48] [49] [50] [51] [52] pd_controller.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/script_engine/director/pd_controller.py
[53] [54] [61] [62] [63] [64] [71] [72] [79] [80] [81] [82] [96] timeline_builder.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/script_engine/persona/timeline_builder.py
[55] [56] [57] [58] [59] [60] [65] [66] [67] [68] [69] [70] [92] [93] [95] [97] [98] openai_writer.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/script_engine/openai_writer.py
[76] [77] [78] rolling_brain.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/script_engine/rolling_brain.py
[83] [84] [85] [86] [87] [88] [89] [90] [91] brain_engine.py
https://github.com/ToknNews/toknnews/blob/5abf4e471d702dbbd064465ba23b002a8758e03e/backend/script_engine/character_brain/brain_engine.py
